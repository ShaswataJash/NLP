{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KnowledgeGraphCreationExampleFromBioMedicalAbstract.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPYj9DZPj0WLbUWX9mal0fP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaswataJash/NLP/blob/master/KnowledgeGraphCreationExampleFromBioMedicalAbstract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxuZhJcUo6hA",
        "colab_type": "text"
      },
      "source": [
        "**Install Scientific spacy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCDCaFiT09nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install scispacy==0.2.5\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_lg-0.2.5.tar.gz\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_ner_craft_md-0.2.5.tar.gz\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_ner_jnlpba_md-0.2.5.tar.gz\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_ner_bc5cdr_md-0.2.5.tar.gz\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_ner_bionlp13cg_md-0.2.5.tar.gz  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99RvAdKPphvR",
        "colab_type": "text"
      },
      "source": [
        "**Install AllenNLP to access the neural-model for Coref resolution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSfp_XhBJCM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install allennlp==1.1.0rc4 allennlp-models==1.1.0rc4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZp6TWeXoLUp",
        "colab_type": "text"
      },
      "source": [
        "**If you want to try out hugginFace's neural-coref for coreference resolution, please execute following two notebook cells**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LraZF7DvrDvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -Rf neuralcoref\n",
        "!git clone https://github.com/huggingface/neuralcoref.git\n",
        "!pip install ./neuralcoref"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zD34Mup5X4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://s3.amazonaws.com/models.huggingface.co/neuralcoref/neuralcoref.tar.gz ~/.neuralcoref_cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOn2yvgk94p9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gdown==3.6.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt5GtTadptEJ",
        "colab_type": "text"
      },
      "source": [
        "**Please acquire UMLS license to download 2020AA UMLS Metathesaurus Files - 'MRCONSO.RRF' (refer: https://www.nlm.nih.gov/research/umls/licensedcontent/umlsknowledgesources.html)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgekLaYz94nG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#downloading from my personal Google drive\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1dXYSVmb_R1-vvFORJXtbeHT1VQTjwrzM', 'umls-2020AA-mrconso.zip', quiet=False) #426MB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbWytQgS-Ihu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "import time\n",
        "#ref: https://www.nlm.nih.gov/research/umls/licensedcontent/umlsknowledgesources.html\n",
        "\n",
        "#unpacking of this zip file may take more than 30 seconds\n",
        "start_time_of_unpacking = time.time()\n",
        "shutil.unpack_archive('umls-2020AA-mrconso.zip', '.')\n",
        "print(\"total time taken for unpacking = %s in seconds\" % (time.time() - start_time_of_unpacking))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAPIdnbFqUTr",
        "colab_type": "text"
      },
      "source": [
        "**Creating a token to CUI mapping from 'MRCONSO.RRF'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw4FXuJJhs0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import tqdm\n",
        "import re\n",
        "\n",
        "def myTokenizer(my_input, find_tokens_from_hyphenated_word=False):\n",
        "    valid_tokens = set()\n",
        "    tokens_with_hyphen = []\n",
        "\n",
        "    text = re.sub(' +',' ',my_input) #without extra spaces\n",
        "    text = text.replace (' -', '-') #convert to tight hyphen\n",
        "    text = text.replace ('- ', '-') #convert to tight hyphen\n",
        "    phrase = text.replace(',', ' ').replace(';', ' ') \\\n",
        "            .replace('(', '') \\\n",
        "            .replace(')', '') \\\n",
        "            .replace(':', '') \\\n",
        "            .replace('%', '') \\\n",
        "            .replace('{', '') \\\n",
        "            .replace('}', '') \\\n",
        "            .replace('[', '') \\\n",
        "            .replace(']', '') \\\n",
        "            .strip()\n",
        "    tokens = phrase.split(' ') #first split token according to space\n",
        "    if len(tokens) <= 1:\n",
        "        if '-' in tokens[0]:\n",
        "            tokens_with_hyphen.extend(tokens[0].split('-'))\n",
        "        else:\n",
        "            return None\n",
        "    \n",
        "    if len(tokens) > 1:\n",
        "        for t in tokens:\n",
        "            if len(t) <= 1:\n",
        "                continue #single char tokens ignored e.g. %, blank tokens ignored (if consecutive multi spaces were present)\n",
        "            try:\n",
        "                float(t)\n",
        "                continue #valid number including floating numbers are ignored\n",
        "            except ValueError:\n",
        "                valid_tokens.add(t)\n",
        "                if '-' in t:\n",
        "                    tokens_with_hyphen.extend(t.split('-'))\n",
        "\n",
        "    if find_tokens_from_hyphenated_word:\n",
        "        for t in tokens_with_hyphen:\n",
        "            if len(t) <= 1:\n",
        "                continue #single char tokens ignored e.g. %, blank tokens ignored (if consecutive multi spaces were present)\n",
        "            try:\n",
        "                float(t)\n",
        "                continue #valid number including floating numbers are ignored\n",
        "            except ValueError:\n",
        "                valid_tokens.add(t)\n",
        "\n",
        "    return valid_tokens\n",
        "\n",
        "\n",
        "umls_header_columns = ['CUI','LAT','TS', 'LUI', 'STT', 'SUI', 'ISPREF', 'AUI', 'SAUI', 'SCUI', 'SDUI', 'SAB', 'TTY', 'CODE', 'STR', 'SRL', 'SUPPRESS', 'CVF']\n",
        "\n",
        "class UmlsMapper:\n",
        "\n",
        "    def __init__(self, isCuiToLongNameMappingReqd=False):\n",
        "        self.umls_dict = {}\n",
        "        multi_token_to_cui_mapping = defaultdict(set)\n",
        "        self.cui_to_minimal_text = {} \n",
        "        self.cui_to_maximal_text = {} \n",
        "         \n",
        "        with open('MRCONSO.RRF', 'r', encoding='utf-8') as umls_file:\n",
        "            for line in tqdm.tqdm(umls_file):\n",
        "                umls_columns = line.split('|')\n",
        "                lang = umls_columns[umls_header_columns.index('LAT')]\n",
        "                if lang != 'ENG':\n",
        "                    continue\n",
        "                cui = umls_columns[umls_header_columns.index('CUI')]\n",
        "                name = umls_columns[umls_header_columns.index('STR')].lower()\n",
        "                self.umls_dict[name] = cui\n",
        "\n",
        "                if cui in self.cui_to_minimal_text:\n",
        "                    if len(name) < len(self.cui_to_minimal_text[cui]):\n",
        "                        self.cui_to_minimal_text[cui] = name\n",
        "                else:\n",
        "                    self.cui_to_minimal_text[cui] = name\n",
        "\n",
        "                if isCuiToLongNameMappingReqd:\n",
        "                    if cui in self.cui_to_maximal_text:\n",
        "                        if len(name) > len(self.cui_to_maximal_text[cui]):\n",
        "                            self.cui_to_maximal_text[cui] = name\n",
        "                    else:\n",
        "                        self.cui_to_maximal_text[cui] = name\n",
        "\n",
        "                #now check whether umls str is multi token\n",
        "                multi_token = myTokenizer(name)\n",
        "                if not (multi_token is None):\n",
        "                    for token in multi_token:\n",
        "                        multi_token_to_cui_mapping[token].add(cui)\n",
        "\n",
        "        self.__umls_original_key_count = len(self.umls_dict)\n",
        "\n",
        "        #now make a second pass to remove tokens which belonged to different CUI\n",
        "        for token,cuis in multi_token_to_cui_mapping.items():\n",
        "            if len(cuis) == 1:\n",
        "                if not (token in self.umls_dict):\n",
        "                    if token[0].isalnum() == False:\n",
        "                        print(token,cuis)\n",
        "                    self.umls_dict[token] = next(iter(cuis))\n",
        "\n",
        "        del multi_token_to_cui_mapping\n",
        "\n",
        "    def getOriginalKeysCount(self):\n",
        "        return self.__umls_original_key_count\n",
        "         \n",
        "    def getKeysCount(self):\n",
        "        return len(self.umls_dict)\n",
        "\n",
        "    def getTokenToUmlsMapping(self, text, verbose=0):\n",
        "        text = text.lower()\n",
        "        try:\n",
        "            return set([self.umls_dict[text]])\n",
        "        except KeyError as e:\n",
        "            if verbose > 0: print('getTokenToUmlsMapping(): <%s>' % (text))\n",
        "            multi_token = myTokenizer(text)\n",
        "            res = set()\n",
        "            if not (multi_token is None):\n",
        "                for token in multi_token:\n",
        "                    try:\n",
        "                        res.add(self.umls_dict[token])\n",
        "                    except KeyError as e:\n",
        "                        pass\n",
        "\n",
        "            return res\n",
        "\n",
        "    def getCUIToShortName(self,cui):\n",
        "        try:\n",
        "            return self.cui_to_minimal_text[cui]\n",
        "        except KeyError as e:\n",
        "            return ''\n",
        "\n",
        "    def getCUIToLongName(self,cui):\n",
        "        try:\n",
        "            return self.cui_to_maximal_text[cui]\n",
        "        except KeyError as e:\n",
        "            return ''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tELqnrrK-Prz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "umlsMapper = UmlsMapper()\n",
        "print('getOriginalKeysCount=',umlsMapper.getOriginalKeysCount())\n",
        "print('getKeysCount=',umlsMapper.getKeysCount())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIJ7cmj1qxJJ",
        "colab_type": "text"
      },
      "source": [
        "**Few sample abstracts of BioMedical scientific papers** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qxFcTsHf0_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pprint\n",
        "t1 = 'Inflammatory diseases of the respiratory tract are commonly associated with elevated production of nitric oxide (NOâ€¢) and increased indices of NOâ€¢ -dependent oxidative stress. Although NOâ€¢ is known to have anti-microbial, anti-inflammatory and anti-oxidant properties, various lines of evidence support the contribution of NOâ€¢ to lung injury in several disease models. On the basis of biochemical evidence, it is often presumed that such NOâ€¢ -dependent oxidations are due to the formation of the oxidant peroxynitrite, although alternative mechanisms involving the phagocyte-derived heme proteins myeloperoxidase and eosinophil peroxidase might be operative during conditions of inflammation. Because of the overwhelming literature on NOâ€¢ generation and activities in the respiratory tract, it would be beyond the scope of this commentary to review this area comprehensively. Instead, it focuses on recent evidence and concepts of the presumed contribution of NOâ€¢ to inflammatory diseases of the lung.'.encode('cp1252').decode('utf-8')\n",
        "pprint.pprint(t1, width=120)\n",
        "t2 = 'Heterogeneous nuclear ribonucleoprotein (hnRNP A1) is involved in pre-mRNA splicing in the nucleus and translational regulation in the cytoplasm. In the present study, we demonstrate that hnRNP A1 also participates in the transcription and replication of a cytoplasmic RNA virus, mouse hepatitis virus (MHV). Overexpression of hnRNP A1 accelerated the kinetics of viral RNA synthesis, whereas the expression in the cytoplasm of a dominant-negative hnRNP A1 mutant that lacks the nuclear transport domain significantly delayed it. The hnRNP A1 mutant caused a global inhibition of viral mRNA transcription and genomic replication, and also a preferential inhibition of the replication of defective-interfering RNAs. Similar to the wild-type hnRNP A1, the hnRNP A1 mutant complexed with an MHV polymerase gene product, the nucleocapsid protein and the viral RNA. However, in contrast to the wild-type hnRNP A1, the mutant protein failed to bind a 250 kDa cellular protein, suggesting that the recruitment of cellular proteins by hnRNP A1 is important for MHV RNA synthesis. Our findings establish the importance of cellular factors in viral RNA-dependent RNA synthesis.'.encode('cp1252').decode('utf-8')\n",
        "pprint.pprint(t2, width=120)\n",
        "t3 = 'Recent evidence suggests that critically ill patients are able to tolerate lower levels of haemoglobin than was previously believed. It is our goal to show that transfusing to a level of 100 g/l does not improve mortality and other clinically important outcomes in a critical care setting. Although many questions remain, many laboratory and clinical studies, including a recent randomized controlled trial (RCT), have established that transfusing to normal haemoglobin concentrations does not improve organ failure and mortality in the critically ill patient. In addition, a restrictive transfusion strategy will reduce exposure to allogeneic transfusions, result in more efficient use of red blood cells (RBCs), save blood overall, and decrease health care costs.'.encode('cp1252').decode('utf-8')\n",
        "pprint.pprint(t3, width=120)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU1vhRm9q-zy",
        "colab_type": "text"
      },
      "source": [
        "*   Two different ways of Abbreviation resolver\n",
        "*   Two different ways of Coreference resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0v5iQ3USY1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def cleanTexts(original_text):\n",
        "    text = original_text\n",
        "    text = re.sub(' +',' ',text) #without extra spaces\n",
        "    #Note hyphens which are placed tightly between two words (i.e. without any space) are fine\n",
        "    text = text.replace (' -', '-') #remove extra space before hyphen\n",
        "    text = text.replace ('- ', '-') #remove extra space after hyphen \n",
        "    return text    \n",
        "\n",
        "#Much faster implementation than scispacy's AbbreviationDetector\n",
        "#Additionally, scispacy does not able to correctly resolve abbreviation for (hnRNP A1) in t2\n",
        "def myResolvedAbbreviation(original_text, verbose=0):\n",
        "    current_state = 0\n",
        "    abbr_map = {}\n",
        "    token_list = []\n",
        "    current_token = ''\n",
        "    for c in original_text:\n",
        "        if current_state == 0:\n",
        "            if c == '(':\n",
        "                if len(current_token) > 0:\n",
        "                    token_list.append(current_token)\n",
        "                current_token = ''\n",
        "                current_state = 1\n",
        "            elif c.isspace():\n",
        "                if len(current_token) > 0:\n",
        "                    token_list.append(current_token.lower())\n",
        "                current_token = ''\n",
        "            else:\n",
        "                current_token += c\n",
        "\n",
        "        elif  current_state == 1:\n",
        "            if c == ')':\n",
        "                current_token = current_token.strip()\n",
        "                assert len(current_token) > 0\n",
        "                abbr_original = current_token\n",
        "                abbr = current_token.lower()\n",
        "                first_char_in_abbr = abbr[0]\n",
        "                count_of_first_char = abbr.count(first_char_in_abbr)\n",
        "                first_char_observed = 0\n",
        "                abbr_resolved = []\n",
        "                for t in reversed(token_list):\n",
        "                    abbr_resolved.append(t)\n",
        "                    if t[0] == first_char_in_abbr:\n",
        "                        first_char_observed += 1\n",
        "                        if first_char_observed == count_of_first_char:\n",
        "                            abbr_map[abbr_original] = ' '.join(reversed(abbr_resolved))\n",
        "                            if verbose>0:\n",
        "                                print(abbr_original,abbr_map[abbr_original])\n",
        "                            break\n",
        "                token_list.clear()\n",
        "                current_state = 0\n",
        "            else:\n",
        "                current_token += c\n",
        "\n",
        "    abrreviatedReplacedText = original_text\n",
        "    for abrv,abrv_resolved in abbr_map.items():\n",
        "        abrreviatedReplacedText = abrreviatedReplacedText.replace(abrv, '', 1) #remove first occurence of abbreviation\n",
        "        abrreviatedReplacedText = abrreviatedReplacedText.replace('()','')\n",
        "        abrreviatedReplacedText = abrreviatedReplacedText.replace(abrv,abrv_resolved)\n",
        "    return abrreviatedReplacedText\n",
        "\n",
        "\n",
        "def resolveAbbreviation(original_text, verbose=0):\n",
        "    from scispacy.abbreviation import AbbreviationDetector\n",
        "    import en_core_sci_lg\n",
        "\n",
        "    nlp = en_core_sci_lg.load() #refer: https://allenai.github.io/scispacy/\n",
        "    \n",
        "    # Add the abbreviation pipe to the spacy pipeline.\n",
        "    abbreviation_pipe = AbbreviationDetector(nlp)\n",
        "    nlp.add_pipe(abbreviation_pipe)\n",
        "\n",
        "    doc = nlp(original_text)\n",
        "    abbrMap = {}\n",
        "    abrreviatedReplacedText = original_text\n",
        "    for abrv in doc._.abbreviations:\n",
        "        if str(abrv) not in abbrMap:\n",
        "            abbrMap[str(abrv)] = str(abrv._.long_form)\n",
        "            if verbose >= 1:\n",
        "                print(\"Abbr found=%s %s\" % (str(abrv),str(abrv._.long_form)))\n",
        "            abrreviatedReplacedText = abrreviatedReplacedText.replace(str(abrv), '', 1) #remove first occurence of abbreviation\n",
        "            abrreviatedReplacedText = abrreviatedReplacedText.replace('()','')\n",
        "            abrreviatedReplacedText = abrreviatedReplacedText.replace(str(abrv),str(abrv._.long_form))\n",
        "        else:\n",
        "            assert abbrMap[str(abrv)] == str(abrv._.long_form)\n",
        "    return abrreviatedReplacedText\n",
        "\n",
        "#AllenNLP models does correct coref resolution for 't2'\n",
        "def resolvePronounCorefUsingAllenNLP(original_text, verbose=0):\n",
        "    from allennlp.predictors.predictor import Predictor\n",
        "    import allennlp_models.coref\n",
        "    predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
        "    result = predictor.predict(document=original_text)\n",
        "    \n",
        "    coref_map = {}\n",
        "    for cluster in result['clusters']:\n",
        "        main_group_extracted = ''\n",
        "        for group in cluster:\n",
        "            assert len(group) == 2\n",
        "            start_token_of_group = group[0]\n",
        "            end_token_of_group = group[-1]\n",
        "            if main_group_extracted == '':\n",
        "                for t in range(start_token_of_group,end_token_of_group+1):\n",
        "                    main_group_extracted = main_group_extracted + result['document'][t] + ' '\n",
        "                if verbose > 0:print(main_group_extracted)\n",
        "            else:\n",
        "                if (end_token_of_group - start_token_of_group) == 0:\n",
        "                    coref_map[start_token_of_group] = main_group_extracted\n",
        "                    if verbose > 0:print(result['document'][start_token_of_group], start_token_of_group, '=>', main_group_extracted.strip())\n",
        "\n",
        "    coref_resolved_text = ''\n",
        "    for token_index,token in enumerate(result['document']):\n",
        "        if token_index in coref_map:\n",
        "            coref_resolved_text = coref_resolved_text + coref_map[token_index] + ' '\n",
        "        else:\n",
        "            coref_resolved_text = coref_resolved_text + token + ' '\n",
        "\n",
        "    return coref_resolved_text\n",
        "\n",
        "def resolvePronounCorefUsingNeuralCoref(original_text, greedyness=0.5):\n",
        "    import neuralcoref\n",
        "    import en_core_sci_lg\n",
        "\n",
        "    nlp = en_core_sci_lg.load()\n",
        "    #add coreference resolver in pipeline\n",
        "    coref = neuralcoref.NeuralCoref(nlp.vocab, greedyness=greedyness)\n",
        "    nlp.add_pipe(coref, name='neuralcoref')\n",
        "    doc = nlp(original_text)\n",
        "    #https://www.rangakrish.com/index.php/2019/02/03/coreference-resolution-using-spacy/\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'PRON' and token._.in_coref:\n",
        "            for cluster in token._.coref_clusters:\n",
        "                print(token.text + ' => ' + cluster.main.text)\n",
        "    return doc._.coref_resolved #co-reference resolved (pronoun resolved with target noun)\n",
        "\n",
        "cleanedText = cleanTexts(t3)\n",
        "#abrreviatedReplacedText = resolveAbbreviation(cleanedText, verbose=1)\n",
        "abrreviatedReplacedText = myResolvedAbbreviation(cleanedText, verbose=1)\n",
        "pprint.pprint(\"abbr  resolved text=\\n%s\" % (abrreviatedReplacedText),  width=120)\n",
        "#corefResolvedText = resolvePronounCorefUsingNeuralCoref(abrreviatedReplacedText)\n",
        "corefResolvedText = resolvePronounCorefUsingAllenNLP(abrreviatedReplacedText, verbose=1)\n",
        "pprint.pprint(\"coref resolved text=\\n%s\" % (corefResolvedText), width=120)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5G8g_3zsUG7",
        "colab_type": "text"
      },
      "source": [
        "**Class holding end-to-end NLP pipeline logic for extracting concepts and relationship tuple between them.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_-V9kc5p2TA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_sci_lg\n",
        "import en_ner_craft_md #GGP, SO, TAXON, CHEBI, GO, CL\n",
        "import en_ner_jnlpba_md #DNA, CELL_TYPE, CELL_LINE, RNA, PROTEIN\n",
        "import en_ner_bc5cdr_md #DISEASE, CHEMICAL\n",
        "import en_ner_bionlp13cg_md #CANCER, ORGAN, TISSUE, ORGANISM, CELL, AMINO_ACID, GENE_OR_GENE_PRODUCT, SIMPLE_CHEMICAL, ANATOMICAL_SYSTEM, IMMATERIAL_ANATOMICAL_ENTITY, MULTI-TISSUE_STRUCTURE, DEVELOPING_ANATOMICAL_STRUCTURE, ORGANISM_SUBDIVISION, CELLULAR_COMPONENT\n",
        "\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "import pprint\n",
        "import itertools\n",
        "\n",
        "class NERRelationshipExtraction:\n",
        "\n",
        "    def __init__(self):\n",
        "    \n",
        "        self.__predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
        "\n",
        "        self.__nlp_en_ner_craft_md = en_ner_craft_md.load()\n",
        "        self.__nlp_en_ner_jnlpba_md = en_ner_jnlpba_md.load()\n",
        "        self.__nlp_en_ner_bc5cdr_md = en_ner_bc5cdr_md.load()\n",
        "        self.__nlp_en_ner_bionlp13cg_md = en_ner_bionlp13cg_md.load()\n",
        "        self.__nlp_en_core_sci_lg = en_core_sci_lg.load()\n",
        "\n",
        "        self.__nlp_en_ner_craft_md.add_pipe(self.__nlp_en_ner_craft_md.create_pipe(\"merge_entities\"))\n",
        "        self.__nlp_en_ner_jnlpba_md.add_pipe(self.__nlp_en_ner_jnlpba_md.create_pipe(\"merge_entities\"))\n",
        "        self.__nlp_en_ner_bc5cdr_md.add_pipe(self.__nlp_en_ner_bc5cdr_md.create_pipe(\"merge_entities\"))\n",
        "        self.__nlp_en_ner_bionlp13cg_md.add_pipe(self.__nlp_en_ner_bionlp13cg_md.create_pipe(\"merge_entities\"))\n",
        "        self.__nlp_en_core_sci_lg.add_pipe(self.__nlp_en_core_sci_lg.create_pipe(\"merge_entities\"))\n",
        "\n",
        "    @staticmethod\n",
        "    def __cleanTexts(original_text):\n",
        "        text = original_text\n",
        "        text = re.sub(' +',' ',text) #without extra spaces\n",
        "        #Note hyphens which are placed tightly between two words (i.e. without any space) are fine\n",
        "        text = text.replace (' -', '-') #remove extra space before hyphen\n",
        "        text = text.replace ('- ', '-') #remove extra space after hyphen \n",
        "        return text    \n",
        "\n",
        "    @staticmethod\n",
        "    def __myResolvedAbbreviation(original_text, verbose=0):\n",
        "        current_state = 0\n",
        "        abbr_map = {}\n",
        "        token_list = []\n",
        "        current_token = ''\n",
        "        for c in original_text:\n",
        "            if current_state == 0:\n",
        "                if c == '(':\n",
        "                    if len(current_token) > 0:\n",
        "                        token_list.append(current_token)\n",
        "                    current_token = ''\n",
        "                    current_state = 1\n",
        "                elif c.isspace():\n",
        "                    if len(current_token) > 0:\n",
        "                        token_list.append(current_token.lower())\n",
        "                    current_token = ''\n",
        "                else:\n",
        "                    current_token += c\n",
        "\n",
        "            elif  current_state == 1:\n",
        "                if c == ')':\n",
        "                    current_token = current_token.strip()\n",
        "                    assert len(current_token) > 0\n",
        "                    abbr_original = current_token\n",
        "                    abbr = current_token.lower()\n",
        "                    first_char_in_abbr = abbr[0]\n",
        "                    count_of_first_char = abbr.count(first_char_in_abbr)\n",
        "                    first_char_observed = 0\n",
        "                    abbr_resolved = []\n",
        "                    for t in reversed(token_list):\n",
        "                        abbr_resolved.append(t)\n",
        "                        if t[0] == first_char_in_abbr:\n",
        "                            first_char_observed += 1\n",
        "                            if first_char_observed == count_of_first_char:\n",
        "                                abbr_map[abbr_original] = ' '.join(reversed(abbr_resolved))\n",
        "                                if verbose>0:print(abbr_original,abbr_map[abbr_original])\n",
        "                                break\n",
        "                    token_list.clear()\n",
        "                    current_state = 0\n",
        "                else:\n",
        "                    current_token += c\n",
        "\n",
        "        abrreviatedReplacedText = original_text\n",
        "        for abrv,abrv_resolved in abbr_map.items():\n",
        "            abrreviatedReplacedText = abrreviatedReplacedText.replace(abrv, '', 1) #remove first occurence of abbreviation\n",
        "            abrreviatedReplacedText = abrreviatedReplacedText.replace('()','')\n",
        "            abrreviatedReplacedText = abrreviatedReplacedText.replace(abrv,abrv_resolved)\n",
        "        return abrreviatedReplacedText\n",
        "\n",
        "    def __resolvePronounCorefUsingAllenNLP(self, original_text, verbose=0):\n",
        "    \n",
        "        result = self.__predictor.predict(document=original_text)\n",
        "    \n",
        "        coref_map = {}\n",
        "        for cluster in result['clusters']:\n",
        "            main_group_extracted = ''\n",
        "            for group in cluster:\n",
        "                assert len(group) == 2\n",
        "                start_token_of_group = group[0]\n",
        "                end_token_of_group = group[-1]\n",
        "                if main_group_extracted == '':\n",
        "                    for t in range(start_token_of_group,end_token_of_group+1):\n",
        "                        main_group_extracted = main_group_extracted + result['document'][t] + ' '\n",
        "                    if verbose > 0:print(main_group_extracted)\n",
        "                else:\n",
        "                    if (end_token_of_group - start_token_of_group) == 0:\n",
        "                        coref_map[start_token_of_group] = main_group_extracted\n",
        "                        if verbose > 0:print(result['document'][start_token_of_group], start_token_of_group, '=>', main_group_extracted.strip())\n",
        "\n",
        "        coref_resolved_text = ''\n",
        "        for token_index,token in enumerate(result['document']):\n",
        "            if token_index in coref_map:\n",
        "                coref_resolved_text = coref_resolved_text + coref_map[token_index] + ' '\n",
        "            else:\n",
        "                coref_resolved_text = coref_resolved_text + token + ' '\n",
        "\n",
        "        return coref_resolved_text\n",
        "    \n",
        "    @staticmethod\n",
        "    def __extract_entities(doc, umlsMapper, toRender):\n",
        "        if toRender:\n",
        "            displacy.render(doc, jupyter=True,style='ent')\n",
        "            for s in doc.sents:\n",
        "                displacy.render(s, jupyter=True,style='dep')\n",
        "        ent_count = defaultdict(int)\n",
        "        sent_to_ent = []\n",
        "    \n",
        "        for s in doc.sents:\n",
        "            noun_in_current_sent = set()\n",
        "            sent_to_token_detailed_info = []\n",
        "            for token in s:\n",
        "                if toRender:\n",
        "                    sent_to_token_detailed_info.append([token.text, token.lemma_, token.pos_, token.tag_, token.dep_])\n",
        "                if token.pos_ == 'NOUN':\n",
        "                    noun_in_current_sent.add(token.text.lower().strip())\n",
        "            if toRender:\n",
        "                table = tabulate(sent_to_token_detailed_info, headers=['text', 'lemma', 'pos', 'tag', 'dep'], tablefmt='orgtbl')\n",
        "                print(table)\n",
        "\n",
        "            ent_in_current_sent = set()\n",
        "            for X in s.ents:\n",
        "                entityInLowerCase = X.text.lower().strip()\n",
        "                if entityInLowerCase in noun_in_current_sent:\n",
        "                    entToCUIMapped = umlsMapper.getTokenToUmlsMapping(entityInLowerCase)\n",
        "                    if len(entToCUIMapped) >= 1:\n",
        "                        for c in entToCUIMapped:\n",
        "                            ent_count[c] += 1\n",
        "                            ent_in_current_sent.add(c)\n",
        "                    else: #no appropriate mapping in UMLS\n",
        "                        ent_count[entityInLowerCase] += 1\n",
        "                        ent_in_current_sent.add(entityInLowerCase)\n",
        "\n",
        "            if len(ent_in_current_sent) > 1:\n",
        "                sent_to_ent.append(ent_in_current_sent)\n",
        "        \n",
        "        return ent_count,sent_to_ent\n",
        "\n",
        "    def getEntityAndEntRelationshipByApplyingAllModels(self, texts_map, umlsMapper, toRender=False, verbosity=0):\n",
        "\n",
        "        texts = []\n",
        "        docids = []\n",
        "        for docid,root_text in texts_map.items():\n",
        "            cleanedText = NERRelationshipExtraction.__cleanTexts(root_text)\n",
        "            abrreviatedReplacedText = NERRelationshipExtraction.__myResolvedAbbreviation(cleanedText, verbose=verbosity)\n",
        "            if verbosity> 0: pprint.pprint(\"Doc-id=%s abbr  resolved text=\\n%s\" % (docid,abrreviatedReplacedText),  width=120)\n",
        "            corefResolvedText = self.__resolvePronounCorefUsingAllenNLP(abrreviatedReplacedText, verbose=verbosity)\n",
        "            if verbosity> 0: pprint.pprint(\"Doc-id=%s coref resolved text=\\n%s\" % (docid,corefResolvedText), width=120)\n",
        "            texts.append(corefResolvedText)\n",
        "            docids.append(docid)\n",
        "\n",
        "        docid_to_ent_info_map = defaultdict(dict)\n",
        "        for docid,doc in zip(docids,self.__nlp_en_ner_craft_md.pipe(texts)):\n",
        "            craft_md_entities_count, craft_md_sent_to_ent = NERRelationshipExtraction.__extract_entities(doc,umlsMapper,toRender)\n",
        "            if verbosity > 0:\n",
        "                print(\"Doc-id=\",docid,\"craft_md Entity count:\",pprint.pformat(craft_md_entities_count))\n",
        "                print(\"Doc-id=\",docid,\"craft_md Sent to ent map:\",pprint.pformat(craft_md_sent_to_ent))\n",
        "            docid_to_ent_info_map[docid]['craft_md_entities_count'] = craft_md_entities_count\n",
        "            docid_to_ent_info_map[docid]['craft_md_sent_to_ent'] = craft_md_sent_to_ent\n",
        "\n",
        "        for docid,doc in zip(docids,self.__nlp_en_ner_jnlpba_md.pipe(texts)):\n",
        "            jnlpba_md_entities_count, jnlpba_md_sent_to_ent = NERRelationshipExtraction.__extract_entities(doc,umlsMapper,toRender)\n",
        "            if verbosity > 0:\n",
        "                print(\"Doc-id=\",docid,\"jnlpba_mdEntity count:\",pprint.pformat(jnlpba_md_entities_count))\n",
        "                print(\"Doc-id=\",docid,\"jnlpba_md Sent to ent map:\",pprint.pformat(jnlpba_md_sent_to_ent))\n",
        "            docid_to_ent_info_map[docid]['jnlpba_md_entities_count'] = jnlpba_md_entities_count\n",
        "            docid_to_ent_info_map[docid]['jnlpba_md_sent_to_ent'] = jnlpba_md_sent_to_ent\n",
        "\n",
        "        for docid,doc in zip(docids,self.__nlp_en_ner_bc5cdr_md.pipe(texts)):\n",
        "            bc5cdr_md_entities_count, bc5cdr_md_sent_to_ent = NERRelationshipExtraction.__extract_entities(doc,umlsMapper,toRender)\n",
        "            if verbosity > 0:\n",
        "                print(\"Doc-id=\",docid,\"bc5cdr_md Entity count:\",pprint.pformat(bc5cdr_md_entities_count))\n",
        "                print(\"Doc-id=\",docid,\"bc5cdr_md Sent to ent map:\",pprint.pformat(bc5cdr_md_sent_to_ent))\n",
        "            docid_to_ent_info_map[docid]['bc5cdr_md_entities_count'] = bc5cdr_md_entities_count\n",
        "            docid_to_ent_info_map[docid]['bc5cdr_md_sent_to_ent'] = bc5cdr_md_sent_to_ent\n",
        "\n",
        "        for docid,doc in zip(docids,self.__nlp_en_ner_bionlp13cg_md.pipe(texts)):\n",
        "            bionlp13cg_md_entities_count, bionlp13cg_md_sent_to_ent = NERRelationshipExtraction.__extract_entities(doc,umlsMapper,toRender)\n",
        "            if verbosity > 0:\n",
        "                print(\"Doc-id=\",docid,\"bionlp13cg_md Entity count:\",pprint.pformat(bionlp13cg_md_entities_count))\n",
        "                print(\"Doc-id=\",docid,\"bionlp13cg_md Sent to ent map:\",pprint.pformat(bionlp13cg_md_sent_to_ent))\n",
        "            docid_to_ent_info_map[docid]['bionlp13cg_md_entities_count'] = bionlp13cg_md_entities_count\n",
        "            docid_to_ent_info_map[docid]['bionlp13cg_md_sent_to_ent'] = bionlp13cg_md_sent_to_ent\n",
        "\n",
        "        for docid,doc in zip(docids,self.__nlp_en_core_sci_lg.pipe(texts)):\n",
        "            core_sci_lg_entities_count, core_sci_lg_sent_to_ent = NERRelationshipExtraction.__extract_entities(doc,umlsMapper,toRender)\n",
        "            if verbosity > 0:\n",
        "                print(\"Doc-id=\",docid,\"core_sci_lg Entity count:\",pprint.pformat(core_sci_lg_entities_count))\n",
        "                print(\"Doc-id=\",docid,\"core_sci_lg Sent to ent map:\",pprint.pformat(core_sci_lg_sent_to_ent))\n",
        "            docid_to_ent_info_map[docid]['core_sci_lg_entities_count'] = core_sci_lg_entities_count\n",
        "            docid_to_ent_info_map[docid]['core_sci_lg_sent_to_ent'] = core_sci_lg_sent_to_ent\n",
        "\n",
        "        #combine the different entities\n",
        "        docid_to_entity_count_map = {} #map of maps\n",
        "        for docid,ent_info_in_doc in docid_to_ent_info_map.items():\n",
        "            combined_entity_count = {}\n",
        "            for d in [ent_info_in_doc['core_sci_lg_entities_count'],\n",
        "                      ent_info_in_doc['bionlp13cg_md_entities_count'],\n",
        "                      ent_info_in_doc['bc5cdr_md_entities_count'],\n",
        "                      ent_info_in_doc['jnlpba_md_entities_count'],\n",
        "                      ent_info_in_doc['craft_md_entities_count']]:\n",
        "                for k in d.keys():\n",
        "                    if k in combined_entity_count and d[k] > combined_entity_count[k] :\n",
        "                        combined_entity_count[k] = d[k]\n",
        "                    else:\n",
        "                        combined_entity_count[k] = d[k]\n",
        "\n",
        "            if verbosity>0:\n",
        "                print(\"Doc-id=\",docid,\"combined all model Entity count:\")\n",
        "                for ent,count in combined_entity_count.items():\n",
        "                    print(ent,'<',umlsMapper.getCUIToShortName(ent),'><',umlsMapper.getCUIToLongName(ent),'> :', count)\n",
        "\n",
        "            docid_to_entity_count_map[docid] = combined_entity_count\n",
        "\n",
        "        #combine the different ent per sentense\n",
        "        docid_to_sent_to_ent_map = {} #map of maps\n",
        "        for docid,ent_info_in_doc in docid_to_ent_info_map.items():\n",
        "            combined_sent_to_ent_relationship = defaultdict(int)\n",
        "            for d in [ent_info_in_doc['core_sci_lg_sent_to_ent'],\n",
        "                      ent_info_in_doc['bionlp13cg_md_sent_to_ent'],\n",
        "                      ent_info_in_doc['bc5cdr_md_sent_to_ent'],\n",
        "                      ent_info_in_doc['jnlpba_md_sent_to_ent'],\n",
        "                      ent_info_in_doc['craft_md_sent_to_ent']]:\n",
        "                for s_to_ent in d:\n",
        "                    #we are sorting the s_to_ent set, so that combination tuple are also generated in ordered fashion\n",
        "                    #this will ensure same tuples (but unordered) will not be inserted as different key in combined_sent_to_ent_relationship\n",
        "                    all_possible_bituple = list(itertools.combinations(sorted(s_to_ent), 2)) \n",
        "                    for p in all_possible_bituple:\n",
        "                        combined_sent_to_ent_relationship[p] += 1\n",
        "\n",
        "            if verbosity>0:\n",
        "                print(\"Doc-id=\",docid,\"combined all model Entity relationship tuple count:\")\n",
        "                for ent_tuple,count in combined_sent_to_ent_relationship.items():\n",
        "                    print(ent_tuple[0],'<',umlsMapper.getCUIToShortName(ent_tuple[0]),'> - ', ent_tuple[1], '<', umlsMapper.getCUIToShortName(ent_tuple[1]),'> :', count)\n",
        "\n",
        "            docid_to_sent_to_ent_map[docid] = combined_sent_to_ent_relationship\n",
        "\n",
        "        return docid_to_entity_count_map, docid_to_sent_to_ent_map\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E3vZhzlkSbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "\n",
        "if 'nerRelExtract' in globals():\n",
        "    del nerRelExtract\n",
        "gc.collect()\n",
        "\n",
        "nerRelExtract = NERRelationshipExtraction()\n",
        "texts_map = {\n",
        "    1:t1,\n",
        "    2:t2,\n",
        "    3:t3\n",
        "}\n",
        "docid_to_entity_count_map, docid_to_sent_to_ent_map = nerRelExtract.getEntityAndEntRelationshipByApplyingAllModels(texts_map, umlsMapper)\n",
        "\n",
        "global_combined_ent_count = defaultdict(int)\n",
        "for docid,ent_info_in_doc in docid_to_entity_count_map.items():\n",
        "    for ent,count in ent_info_in_doc.items():\n",
        "        global_combined_ent_count[ent] += count\n",
        "\n",
        "for ent,count in global_combined_ent_count.items():\n",
        "    print(ent,'<',umlsMapper.getCUIToShortName(ent),'><',umlsMapper.getCUIToLongName(ent),'> :', count)\n",
        "\n",
        "print(\"==========================================\")\n",
        "\n",
        "global_combined_ent_rel_tuple_count = defaultdict(int)\n",
        "for docid,ent_info_in_doc in docid_to_sent_to_ent_map.items():\n",
        "    for ent_tuple,count in ent_info_in_doc.items():\n",
        "        global_combined_ent_rel_tuple_count[ent_tuple] += count\n",
        "\n",
        "for ent_tuple,count in global_combined_ent_rel_tuple_count.items():\n",
        "    print(ent_tuple[0],'<',umlsMapper.getCUIToShortName(ent_tuple[0]),'> - ', ent_tuple[1], '<', umlsMapper.getCUIToShortName(ent_tuple[1]),'> :', count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxeWFo69sy3t",
        "colab_type": "text"
      },
      "source": [
        "**Just for demonstration - mapping tuples representing related UMLS concept appearing as part of same sentense on network-graph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf4rx0R54oPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install networkx==2.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H548ewB15dkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "G = nx.Graph()\n",
        "print(\"doc count=\", len(docid_to_sent_to_ent_map))\n",
        "for docid, combined_sent_to_ent_relationship in docid_to_sent_to_ent_map.items():\n",
        "    for ed_tuple,count in combined_sent_to_ent_relationship.items():\n",
        "        G.add_edge(ed_tuple[0], ed_tuple[1], weight = count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJoqfHb5l8ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "#nx.draw(G, pos=nx.circular_layout(G), edge_color='b', with_labels=True)\n",
        "nx.draw(G, edge_color='b', with_labels=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}